{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - ML approach to assist Lung Cancer Detection from clinical data \n",
    "\n",
    "#### This research combined three clinical datasets, patient data, nodule count per patient, and nodule size list and feed significant features into a rule based classifier algorithms in order to reduce the false positive of clinical identification of lung cancer.......\n",
    "\n",
    "### The key parts in data analysis:\n",
    "#### 1. Data preparation: merging, cleaning, formatting\n",
    "#### 2. ML and prediction\n",
    "#### 3. Evaluation of predictive models performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the first data set, clean the header, correct data type and alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pandas display all columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the tree datasets\n",
    "url_pd = 'https://wiki.cancerimagingarchive.net/download/attachments/3539039/tcia-diagnosis-data-2012-04-20.xls?version=1&modificationDate=1334930231098&api=v2'\n",
    "url_nsl= 'http://www.via.cornell.edu/lidc/list3.2.csv'\n",
    "url_ncp = 'https://wiki.cancerimagingarchive.net/download/attachments/3539039/lidc-idri%20nodule%20counts%20%286-23-2015%29.xlsx?version=1&modificationDate=1435085651880&api=v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read first data set, pd\n",
    "df_pd = pd.read_excel(url_pd)\n",
    "#Remove the notation of column headers to make it shorter\n",
    "headers = ['TCIA Patient ID', 'Diagnosis at the Patient Level', 'Diagnosis Method', \n",
    "           'Primary tumor site for metastatic disease','Nodule 1 Diagnosis at the Nodule Level',\n",
    "          'Nodule 1 Diagnosis Method at the Nodule Level','Nodule 2 Diagnosis at the Nodule Level',\n",
    "          'Nodule 2 Diagnosis Method at the Nodule Level','Nodule 3 Diagnosis at the Nodule Level',\n",
    "          'Nodule 3 Diagnosis Method at the Nodule Level','Nodule 4 Diagnosis at the Nodule Level',\n",
    "          'Nodule 4 Diagnosis Method at the Nodule Level','Nodule 5 Diagnosis at the Nodule Level',\n",
    "          'Nodule 5 Diagnosis Method at the Nodule Level']\n",
    "df_pd.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected data type of 3 categorical variables\n",
    "df_pd['Diagnosis at the Patient Level'] = df_pd['Diagnosis at the Patient Level'].astype('object')\n",
    "df_pd['Diagnosis Method'] = df_pd['Diagnosis Method'].astype('object')\n",
    "df_pd['Nodule 1 Diagnosis at the Nodule Level'] = df_pd['Nodule 1 Diagnosis at the Nodule Level'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we are using the alias of the dataset(easly interpretable), not the coding of if\n",
    "df_pd.loc[df_pd['Diagnosis at the Patient Level'] == 0, 'Diagnosis at the Patient Level'] = 'unknown'\n",
    "df_pd.loc[df_pd['Diagnosis at the Patient Level'] == 1, 'Diagnosis at the Patient Level'] = 'benign or non-malignant disease'\n",
    "df_pd.loc[df_pd['Diagnosis at the Patient Level'] == 2, 'Diagnosis at the Patient Level'] = 'malignant, primary lung cancer'\n",
    "df_pd.loc[df_pd['Diagnosis at the Patient Level'] == 3, 'Diagnosis at the Patient Level'] = 'malignant metastatic'\n",
    "\n",
    "df_pd.loc[df_pd['Diagnosis Method'] == 0, 'Diagnosis Method'] = 'unknown'\n",
    "df_pd.loc[df_pd['Diagnosis Method'] == 1, 'Diagnosis Method'] = 'review of radiological images to show 2 years of stable nodule'\n",
    "df_pd.loc[df_pd['Diagnosis Method'] == 2, 'Diagnosis Method'] = 'biopsy'\n",
    "df_pd.loc[df_pd['Diagnosis Method'] == 3, 'Diagnosis Method'] = 'surgical resection'\n",
    "df_pd.loc[df_pd['Diagnosis Method'] == 4, 'Diagnosis Method'] = 'progression or response'\n",
    "\n",
    "df_pd.loc[df_pd['Nodule 1 Diagnosis at the Nodule Level'] == 0, 'Nodule 1 Diagnosis at the Nodule Level'] = 'unknown'\n",
    "df_pd.loc[df_pd['Nodule 1 Diagnosis at the Nodule Level'] == 1, 'Nodule 1 Diagnosis at the Nodule Level'] = 'benign or non-malignant disease'\n",
    "df_pd.loc[df_pd['Nodule 1 Diagnosis at the Nodule Level'] == 2, 'Nodule 1 Diagnosis at the Nodule Level'] = 'malignant, primary lung cancer'\n",
    "df_pd.loc[df_pd['Nodule 1 Diagnosis at the Nodule Level'] == 3, 'Nodule 1 Diagnosis at the Nodule Level'] = 'malignant metastatic'\n",
    "## 156 out of 1018 patients has diagnosis result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create binary Response Variable, named 'Cancer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to import numpy nan to fill in nan with \"unknown\" in patient diagnosis\n",
    "from numpy import nan\n",
    "df_pd.loc[df_pd['Diagnosis at the Patient Level']=='unknown', 'Cancer'] = nan\n",
    "df_pd.loc[df_pd['Diagnosis at the Patient Level']=='benign or non-malignant disease', 'Cancer'] = 0\n",
    "df_pd.loc[df_pd['Diagnosis at the Patient Level']=='malignant, primary lung cancer','Cancer' ] = 1\n",
    "df_pd.loc[df_pd['Diagnosis at the Patient Level']=='malignant metastatic', 'Cancer'] = 1\n",
    "df_pd['Cancer'] = df_pd['Cancer'].astype('object')\n",
    "df_pd.shape\n",
    "# Final pd dataset has 157 rows and 15 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting 2nd dataset ncp, left join, and keep only the records that has scanning result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 2nd dataset \"nodule count by patient\", to dataframe \"df_ncp_origin\"\n",
    "df_ncp_origin = pd.read_excel(url_ncp)\n",
    "# Drop Unuseful Columns, drop last row (a sum) \n",
    "df_ncp_origin = df_ncp_origin.drop(columns=['Unnamed: 4', 'Unnamed: 5'])\n",
    "df_ncp = df_ncp_origin.dropna(subset = ['TCIA Patent ID'])\n",
    "df_ncp.columns\n",
    "# Shape is (1018,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge df_ncp (Nodule count per patient 1018 *4) with df_pd (Patient Diagnosis 157 *15), saved in df_ncp_pd\n",
    "df_ncp_pd = pd.merge(df_ncp, df_pd, how='left', left_on='TCIA Patent ID', right_on='TCIA Patient ID')\n",
    "df_ncp_pd.shape\n",
    "## To get a dataframe of 1018 * 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep records that only have a scanning result\n",
    "df_ncp_pd = df_ncp_pd.dropna(subset = ['Cancer'])\n",
    "df_ncp_pd.shape\n",
    "# Generating dataframe of 19 columns but only 131 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connectin 3rd dataset df_nsl (nodule size list), clean and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting data source\n",
    "df_nsl_full = pd.read_csv(url_nsl)\n",
    "# Selet columns that will only be used, saved into dataframe df_nsl \n",
    "df_nsl = df_nsl_full[['case', 'scan','roi', 'volume']]\n",
    "df_nsl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create column 'Patient_case' in df_ncp_pd to be the trim of Patent ID with the same \"four digit\" format, and make both string type\n",
    "df_ncp_pd['Patient_case'] = df_ncp_pd['TCIA Patent ID'].str[-4:]\n",
    "df_ncp_pd['Patient_case'] = df_ncp_pd['Patient_case'].apply(str)\n",
    "# Also make 'case' column in nodule size list dataframe into string\n",
    "df_nsl['case'] = df_nsl['case'].apply(str) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a look at the two data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge df_nsl(nodule size list, 2635 * 4) with df_ncp_pd (131 * 19)\n",
    "df_ncp_pd_nsl = pd.merge(df_nsl, df_ncp_pd, how='right', left_on='case',right_on = 'Patient_case')\n",
    "df_ncp_pd_nsl.shape\n",
    "# Shape is (2635, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep records that only have a scanning result\n",
    "df_ncp_pd_nsl = df_ncp_pd_nsl.dropna(subset = ['Cancer'])\n",
    "df_ncp_pd_nsl.shape\n",
    "# Generating dataframe of 19 columns but only 131 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows that has no nodule size, or rows has largest nodule size\n",
    "largest_volume = (df_ncp_pd_nsl['volume'] == df_ncp_pd_nsl.groupby(['case'])['volume'].transform(max)) \n",
    "no_nodule_size = df_ncp_pd_nsl['volume'].isnull()\n",
    "df = df_ncp_pd_nsl.loc[ largest_volume | no_nodule_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since volumne colum has nan, let's impute them with avg\n",
    "df['volume'].fillna((df['volume'].mean()), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset df_ncp_pd_nsl is finally prepared, saved into \"df\" for modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Somehow the 'case' column can be dropped now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the data to spreadsheet, for examination purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to a local csv file in 'Download' folder\n",
    "export_path = '/Users/dahailiu/Downloads/20181109_1039.csv'\n",
    "df.to_csv(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X = df [['Total Number of Nodules* ','Number of Nodules >=3mm**','Number of Nodules <3mm***', 'volume']]\n",
    "y = df ['Cancer']\n",
    "## y can not be object\n",
    "y=y.astype('int')\n",
    "# default is 75% / 25% train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "tree2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n",
    "print(tree2)\n",
    "tree2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpy = [item for item in X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot_ng as pydot\n",
    "from IPython.display import IFrame\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "with open(\"dt.dot\",\"w\") as dot_data:\n",
    "    export_graphviz(tree2, out_file=dot_data, filled=True, \n",
    "                feature_names = lpy,label = 'all')\n",
    "pydot.graph_from_dot_file(\"dt.dot\").write_png(\"dt.png\")\n",
    "IFrame(\"dt.png\", width = 700, height = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 10, max_features = 3, random_state = 0).fit(X_train, y_train)\n",
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Decision Tree (GBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "GBDT = GradientBoostingClassifier(learning_rate = .1, max_depth = 4, random_state = 0)\n",
    "gbdt = GBDT.fit(X_train, y_train)\n",
    "gbdt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting GBDT\n",
    "#with open(\"gbdt.dot\",\"w\") as dot_data:\n",
    "#    export_graphviz(gbdt, out_file=dot_data, filled=True, \n",
    "#                feature_names = lpy,label = 'all')\n",
    "#pydot.graph_from_dot_file(\"gbdt.dot\").write_png(\"dbdt.png\")\n",
    "#IFrame(\"gbdt.png\", width = 700, height = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrics for Gradient Boosted Decision Tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "gbdt_predicted = gbdt.predict(X_test)\n",
    "confusion_gbdt = confusion_matrix(y_test, gbdt_predicted)\n",
    "print('gradient boost decision tree classifier',  confusion_gbdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Gradient Boost Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "# F1 = 2 * Precision * Recall / (Precision + Recall) \n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, gbdt_predicted)))\n",
    "print('Precision, which matters more: {:.2f}'.format(precision_score(y_test, gbdt_predicted)))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, gbdt_predicted)))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, gbdt_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting ROC Curves\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "y_score_gbdt = gbdt.decision_function(X_test)\n",
    "fpr_gbdt, tpr_gbdt, _ = roc_curve(y_test, y_score_gbdt)\n",
    "roc_auc_gbdt = auc(fpr_gbdt, tpr_gbdt)\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_gbdt, tpr_gbdt, lw=3, label='GBDT ROC curve (area = {:0.2f})'.format(roc_auc_gbdt))\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Today's assignment:\n",
    "1. \n",
    "2. output the final file to a spreadsheet and examine potential problems \n",
    "3. Learn precision recall tradeoff\n",
    "4. Sensitivity is more "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions for Jinglu:\n",
    "1. how to imput na in nodule size 'volumn', use avg?\n",
    "2. Flase positive is the problem we need to tackle, shall we use FPR as an evaluation metrics, or \"precision\"\n",
    "3. Push to Git again "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
